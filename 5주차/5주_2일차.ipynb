{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Networks \n",
    "cb_park@korea.ac.kr\n",
    "\n",
    "MLP -> XOR\n",
    "\n",
    "Cost function / objective function (momentum-> global minimum 피하기 위한 것) \n",
    "\n",
    "1. Activation functions \n",
    "\n",
    "- sigmoid -> layers가 많아질수록 gradient값이 작아지는 단점이 있다.\n",
    "        problems \n",
    "        1) saturated neurons kill the gradients (0 -> 0.25)\n",
    "        2) sigmoid outputs are not zero-centered -> 양수값밖에 존재하지 않는다. 따라서 양수 음수로 가고싶으면 zig zag path \n",
    "        3) exp() is a bit compute expensive  \n",
    "        \n",
    "- tanh \n",
    "        - zero-centered (sigmoid) 단점을 해결해 주고자 나온 것. \n",
    "        - tanh(x) = 2 * sigmoid(x) - 1\n",
    "        problems \n",
    "        1) still kills gradients  (0 -> 1) \n",
    "        \n",
    "- ReLU (Recifier Linear Unit) -> gradient 값이 항상 1  **\n",
    "        - layer-> 값을 많이 쌓다보니 깊은 gradient학습을 할 수 있는 ReLU를 많이 씀\n",
    "        - Does not saturate (in+region) \n",
    "        - Very computationally efficient \n",
    "        - Converges much faster than sigmoidthanh in practice (6x)\n",
    "        - Actually more bilogically plausible than sigmoid \n",
    "        problems\n",
    "        1) Not Zero-centered output\n",
    "        2) An annoyance : dead ReLU를 어쩔 수 없이 가져가게 됨 (-값) -> 죽는 노드(30%) 가 있어도 살아남는 노드가 많도록 많이 만듦 (70%)\n",
    "            - ppl like to initialize ReLU neurons with slightly biases (eg. 0.01) \n",
    "\n",
    "- Leaky ReLU\n",
    "        - 음수부분(Dead ReLU)도 적당히 살아남도록 하자  \n",
    "        - max(0.1x, x) \n",
    "        \n",
    "2. Batch Normalization (논문)\n",
    "        - scaleing 차이 \n",
    "        - 표준정규분포 \n",
    "        \n",
    "        - mini batch -> 범위를 정규화해주는 것  \n",
    "        - input을 제한시켜주는것 \n",
    "        - 모델이 stable해지도록 도와줘서 예측 성능을 높여준다.\n",
    "        - 평균은 0, 분산은 1 \n",
    "        \n",
    "* Normalization layer -> layer normalization \n",
    "* instance normalization -> 컴퓨터 vision에서 많이 쓰임 \n",
    "\n",
    "* Data Preprocessing \n",
    "    - Normalization \n",
    "    - PCA: 변동성이 큰 쪽으로 보여주는 것 \n",
    "    - Whitening \n",
    "\n",
    "3. Optimization Methods \n",
    "\n",
    "* Gradient descent \n",
    "    - Local minimum ( momentum -> local minimum을 피해가기 위해서 )\n",
    "    - Global Minimum \n",
    "    - batch <- 전체 데이터\n",
    "    \n",
    "* mini batch gradient \n",
    "    - (1000개 중에서 100개 뽑아서 gradient descent) -> 오히려 성능이 좋다. (샘플링할 때마다 좀 다르다.)  \n",
    "    - (local minimum의 최저점이 아니라 더 내려갈 수 있는 그래프를 발견할 수 있다.)\n",
    "    - 전체 데이터가 아니라 미니 배치 사이즈를 기반으로 한 것 \n",
    "    \n",
    "* stochastic descent \n",
    "    - 샘플링 하나의 데이터\n",
    "\n",
    "* linear -> f(m+n) = f(m) + f(n) ; 2(m+n) = 2m + 2n ; f(an) = a * f(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Momentum Update\n",
    "- Vanilla gradient descent update\n",
    "x -= learning_rate * d(gradient)x\n",
    "\n",
    "- Momentum update\n",
    "v= mu * v - learning_rate * dx \n",
    "x += v\n",
    "\n",
    "* Adagrad Update \n",
    "- Cache ? \n",
    "- parameter 별로 발생된 gradient 값 (균등하게 공평한 양으로 gradient 값으로 적용)\n",
    "- 과거 gradient 값의 계속적으로 축적 (MSE) -> 포인트 가다가 MSE값이 커짐에 따라 값이 미미해 지므로 중간에 잘 가지 않는다. \n",
    "\n",
    "* RMS Prop Update \n",
    "- 계속 증가하는 단점을 바꾸기 위해서 최근값들의 일부만을 저장하고 이전값들을 삭제하는 것 \n",
    "- Momentum을 사용하지는 않는다. (m, v) \n",
    "- parameter 별로 발생된 gradient 값 (균등하게 공평한 양으로 gradient 값으로 적용)\n",
    "\n",
    "* Adam Update \n",
    "- Adagrad Update 모멘텀 값과 RMS Prop Udate 장점을 합친 것  \n",
    "- Adam Optimizer **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Learning Rate scheduler **\n",
    "   \n",
    "4. Ensemble and Regularization \n",
    "- train, validation(tuning), test \n",
    "- Overfitting\n",
    "- Underfitting \n",
    "\n",
    "* Regularization\n",
    "-> Add term to loss\n",
    "- L2 regularization -> W^2\n",
    "- L1 regularization -> |W|\n",
    "\n",
    "-> Dropout \n",
    "- 앙상블처럼 생각할 수 있다. (?)\n",
    "- 한 변수를 dropout 하면 범위가 달라질 수 있다. (따라서 값을 스케일 up을 해줘야한다.) \n",
    "- test할 때는 dropout을 꺼야한다. \n",
    "- train 때 dropout 을 켜야하는 이유가 스케일 up을 해주기 때문이다. \n",
    "\n",
    "-> Ensemble \n",
    "- 여러 모델을 써서 다수결의 선택으로 선택된 결과값을 고르는 것 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "- Deep stacking \n",
    "convolution network -> ReLU -> pooling \n",
    "\n",
    "- element wise -> 박스를 합쳐서 \n",
    "\n",
    "- Gradient descent -> cost function \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 영상 및 자연어 처리 분야 \n",
    "## 인공지능 기술 동향 및 전망\n",
    "\n",
    "## Deep Learning \n",
    "* 딥러닝의 성공 요인 \n",
    "- Data : Large datasets \n",
    "- Hardware: GPU acceleration\n",
    "- Algorithm: e.g. batch norm, ADAM, attention , ... \n",
    "\n",
    "https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.49446&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false\n",
    "\n",
    "\n",
    "- Augmentation : 뻥튀기\n",
    "    - 상하반전은 보통 하지 않는다. \n",
    "    - 가로, 세로 distortion \n",
    "    \n",
    "- 분야\n",
    "    - 영상 인식 (CV)\n",
    "    - 자연어 처리: 기계 번역, 대화 시스템\n",
    "    - 음성 인식\n",
    "    - 게임 인공 지능\n",
    "    - 의료: 도덕적 문제 -> 책임 \n",
    "    - 법률\n",
    "    - 금융\n",
    "    \n",
    "- NeuralTalk - image captioning \n",
    "- Style Transfer \n",
    "\n",
    "3대 거장: geoffrey hinton, andrew ng, yann lecun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 인공지능 연구의 최신 동향 \n",
    "\n",
    "- 데이터의 생성 및 변환 모델\n",
    "\n",
    "- 벡터 표현형을 통한 이종 데이터의 통합 및 변환\n",
    "\n",
    "- 비지도 및 자가지도 학습 - clustering \n",
    "\n",
    "- 실제 활용될 때의 이슈들 \n",
    "\n",
    "Paintschainer : https://paintschainer.preferred.tech/index_en.html\n",
    "\n",
    "- Every Body can dance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN(image)\n",
    "\n",
    "Google net\n",
    "\n",
    "RNN - LSTN(text) (?) \n",
    "\n",
    "-> 기법에 상관없이 다양하게 쓰이고 있음. \n",
    "\n",
    "Transformer based on Self-Attention : 최근 딥러닝 기반 자연어 처리 모델의 기본 구조로 사용됨 \n",
    "\n",
    "Attention Model\n",
    "\n",
    "BERT \n",
    "\n",
    "GPT2\n",
    "\n",
    "향후 전망 \n",
    "-> 기술 발전 속도는 점점 더뎌지고 있음. \n",
    "**데이터 확보가 중요** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 문제 발견 능력 \n",
    "- 인공지능\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
