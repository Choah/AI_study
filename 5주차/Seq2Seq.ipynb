{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Seq2Seq.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"5zthtOlIjcO1","colab_type":"text"},"source":["**Copyright(C). Cheonbok Park. All rights reserved.**\n","\n","Email : cb_park@korea.ac.kr"]},{"cell_type":"code","metadata":{"id":"bkJFyFk23EbB","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lWPhKRrqw8Vl","colab_type":"text"},"source":["### 필요 라이브러리 설치"]},{"cell_type":"code","metadata":{"id":"krquphUSw64W","colab_type":"code","colab":{}},"source":["# 필요한 라이브러리를 설치합니다. "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TF6swkROGoRk","colab_type":"code","colab":{}},"source":["import torch # torch library \n","import torch.nn as nn # Nueral Network에 대한 package\n","import numpy as np  # numpy \n","import editdistance # 평가 지표로서 사용될 edit distance \n","import matplotlib.pyplot as plt # plot 을 찍기 위한 라이브러리\n","import tqdm\n","import torch.nn.functional as F # pytorch function 들을 사용하기 위한 용도 \n","from torch.utils import data # dataset 관련된 utility 를 사용하려는 용도\n","from random import choice, randrange # random\n","from itertools import zip_longest \n","import librosa\n","import os   # directory 생성 및 디렉토리 생성과 관련된 package \n","import json \n","import random\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dvgO0Z-3xKpr","colab_type":"text"},"source":["### Data Loader "]},{"cell_type":"markdown","metadata":{"id":"N5VY4V8ifgNm","colab_type":"text"},"source":["![대체 텍스트](https://pytorch.org/tutorials/_images/word-encoding.png)"]},{"cell_type":"code","metadata":{"id":"JPt35iPSs0dO","colab_type":"code","colab":{}},"source":["def batch(iterable, n=1):\n","    args = [iter(iterable)] * n\n","    return zip_longest(*args)\n","\n","\n","def pad_tensor(vec, pad, value=0, dim=0):\n","    \"\"\"\n","    pad token으로 채우는 용도 \n","    args:\n","        vec - tensor to pad\n","        pad - the size to pad to\n","        dim - dimension to pad\n","    return:\n","        a new tensor padded to 'pad' in dimension 'dim'\n","    \"\"\"\n","    pad_size = pad - vec.shape[0]\n","\n","    if len(vec.shape) == 2:\n","        zeros = torch.ones((pad_size, vec.shape[-1])) * value\n","    elif len(vec.shape) == 1:\n","        zeros = torch.ones((pad_size,)) * value\n","    else:\n","        raise NotImplementedError\n","    return torch.cat([torch.Tensor(vec), zeros], dim=dim)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7t-W0gQps2rg","colab_type":"text"},"source":["![대체 텍스트](https://i.stack.imgur.com/Kuhh0.jpg)"]},{"cell_type":"code","metadata":{"id":"QAaKIfA7BdLX","colab_type":"code","colab":{}},"source":["def pad_collate(batch, values=(0, 0), dim=0):\n","    \"\"\"\n","    데이터 로더에 들어가기전에 batch화 할 때 거치는 함수 \n","    args:\n","        batch - list of (tensor, label)\n","    reutrn:\n","        xs - a tensor of all examples in 'batch' after padding\n","        ys - a LongTensor of all labels in batch\n","        ws - a tensor of sequence lengths\n","    \"\"\"\n","\n","    sequence_lengths = torch.Tensor([int(x[0].shape[dim]) for x in batch]) # 각 batch 마다 길이를 얻어내고 \n","    sequence_lengths, xids = sequence_lengths.sort(descending=True) # 감소하는 순서로 정렬\n","    target_lengths = torch.Tensor([int(x[1].shape[dim]) for x in batch])\n","    # find longest sequence (가장 긴 sequence의 길이를 구함 )\n","    src_max_len = max(map(lambda x: x[0].shape[dim], batch))\n","    tgt_max_len = max(map(lambda x: x[1].shape[dim], batch))\n","    # pad according to max_len (max length 만큼 padd를 추가 )\n","    batch = [(pad_tensor(x, pad=src_max_len, dim=dim), pad_tensor(y, pad=tgt_max_len, dim=dim)) for (x, y) in batch]\n","\n","    # stack all # legnth same \n","    xs = torch.stack([x[0] for x in batch], dim=0)\n","    ys = torch.stack([x[1] for x in batch], dim=0) # desceding order x \n","    xs = xs[xids].contiguous() # decreasing order(xids)로 다시 나열 \n","    ys = ys[xids].contiguous() # xids 와 같은 순서로 \n","    target_lengths = target_lengths[xids] # same desceding order  \n","    return xs.long(), ys.long(), sequence_lengths.int(), target_lengths.int()\n","\n","\n","class ToyDataset(data.Dataset):\n","    \"\"\"\n","    https://talbaumel.github.io/blog/attention/\n","    \"\"\"\n","    def __init__(self, min_length=5, max_length=20, type='train'):\n","        self.SOS = \"<s>\"  # all strings will end with the End Of String token )\n","        self.EOS = \"</s>\"  # all strings will end with the End Of String token\n","        self.characters = list(\"abcde\")\n","        self.int2char = list(self.characters)\n","        self.char2int = {c: i+3 for i, c in enumerate(self.characters)} # +3 을 왜하는 가?\n","        print(self.char2int)\n","        self.VOCAB_SIZE = len(self.characters)\n","        self.min_length = min_length\n","        self.max_length = max_length\n","        \n","        # train set or test set 을 생성 \n","        if type == 'train':\n","            self.set = [self._sample() for _ in range(4000)]\n","        else:\n","            self.set = [self._sample() for _ in range(300)]\n","\n","    def __len__(self):\n","        # 필수 ! \n","        return len(self.set)\n","\n","    def __getitem__(self, item):\n","        # 필수 !\n","        return self.set[item]\n","\n","    def _sample(self):\n","        random_length = randrange(self.min_length, self.max_length)  # Pick a random length\n","        random_char_list = [choice(self.characters[:-1]) for _ in range(random_length)]  # Pick random chars\n","        random_string = ''.join(random_char_list)\n","        a = np.array([self.char2int.get(x) for x in random_string]+[2])\n","        b = np.array([self.char2int.get(x) for x in random_string[::-1]] + [2]) # Return the random string and its reverse + EOS \n","        \n","        return a, b\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7QcWCIsJzltc","colab_type":"text"},"source":["### utils misc.py\n","\n"]},{"cell_type":"code","metadata":{"id":"SciyvlCzBe-B","colab_type":"code","colab":{}},"source":["EOS_TOKEN = '</s>'\n","\n","\n","def check_size(tensor, *args):\n","    size = [a for a in args]\n","    assert tensor.size() == torch.Size(size), tensor.size()\n","\n","def to_mono(y):\n","    assert y.ndim == 2\n","    return np.mean(y, axis=1)\n","\n","\n","def edit_distance(guess, truth):\n","    guess = guess.split(EOS_TOKEN)[0]\n","    truth = truth[3:].split(EOS_TOKEN)[0]\n","    return editdistance.eval(guess, truth) / len(truth)\n","\n","\n","class AttrDict(dict):\n","  __getattr__ = dict.__getitem__\n","  __setattr__ = dict.__setitem__"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a0Hv4JFBzM-4","colab_type":"text"},"source":["### Edit distance (편집거리 알고리즘) "]},{"cell_type":"markdown","metadata":{"id":"X4ct9e_OzuTT","colab_type":"text"},"source":["![대체 텍스트](https://raw.githubusercontent.com/sumitc91/data/master/askgif-blog/9e07d056-ccf7-4fc8-b6ee-000c8032b9ec_editDistance.gif)"]},{"cell_type":"code","metadata":{"id":"cwbk7k7h4PKB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"92623a26-0f36-4c82-dd91-2a364cd786c6","executionInfo":{"status":"ok","timestamp":1564642054284,"user_tz":-540,"elapsed":809,"user":{"displayName":"Cheonbok Park","photoUrl":"https://lh3.googleusercontent.com/-IBOYs9WOjak/AAAAAAAAAAI/AAAAAAAABAk/gDfiXuRJUhA/s64/photo.jpg","userId":"16050539117346266781"}}},"source":["# edit distance 란 편집 거리 \n","\n","\n","ref = [1, 2, 3, 4]\n","hyp = [1, 2, 4, 5, 6]\n","editdistance.eval(ref,hyp)"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"C1VtDggGzzs_","colab_type":"text"},"source":["### Attention Mask"]},{"cell_type":"code","metadata":{"id":"BOMNQN6Gj_-p","colab_type":"code","colab":{}},"source":["## 추후에 설명 Decoder section\n","def mask_3d(inputs, seq_len, mask_value=0.):\n","    batches = inputs.size()[0]\n","    assert batches == len(seq_len) # length 체크 \n","    max_idx = max(seq_len) # max length 체크 \n","    for n, idx in enumerate(seq_len): # length 에서 의미없는 hidden state attention 값은 0으로 두기 위한 mask값 설정 \n","        if idx < max_idx.item():\n","            if len(inputs.size()) == 3:\n","                inputs[n, idx.int():, :] = mask_value\n","            else:\n","                assert len(inputs.size()) == 2, \"The size of inputs must be 2 or 3, received {}\".format(inputs.size())\n","                inputs[n, idx.int():] = mask_value\n","    return inputs"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kXtIVGUgHmws","colab_type":"text"},"source":["## Encoder RNN"]},{"cell_type":"markdown","metadata":{"id":"LdFDvktffm9k","colab_type":"text"},"source":["![대체 텍스트](https://pytorch.org/tutorials/_images/encoder-network.png)"]},{"cell_type":"markdown","metadata":{"id":"Qf62OoGcwJP5","colab_type":"text"},"source":["#### Embedding Module "]},{"cell_type":"markdown","metadata":{"id":"c8mNg3ve8LmT","colab_type":"text"},"source":["![대체 텍스트](https://i.ibb.co/S0NJzq7/embedding.png)"]},{"cell_type":"markdown","metadata":{"id":"kH2OnZrXwOqk","colab_type":"text"},"source":["#### GRU Module"]},{"cell_type":"markdown","metadata":{"id":"goSiSeiq8bn-","colab_type":"text"},"source":["![대체 텍스트](https://i.ibb.co/881BygH/GRU.png)"]},{"cell_type":"markdown","metadata":{"id":"L_TIF6h58dZf","colab_type":"text"},"source":["![대체 텍스트](https://i.ibb.co/NsMqvcH/GRU-param.png)"]},{"cell_type":"markdown","metadata":{"id":"g-Z_vgd2wSAd","colab_type":"text"},"source":["#### ENCODER RNN Code"]},{"cell_type":"code","metadata":{"id":"8d__61m2HfFR","colab_type":"code","colab":{}},"source":["from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","class EncoderRNN(nn.Module):\n","    def __init__(self, config):\n","        super(EncoderRNN, self).__init__()\n","        self.input_size = config[\"n_channels\"]\n","        self.hidden_size = config[\"encoder_hidden\"]\n","        self.layers = config.get(\"encoder_layers\", 1)\n","        \n","        self.dropout = config.get(\"encoder_dropout\", 0.) \n","        self.bi = config.get(\"bidirectional_encoder\", False)\n","        embedding_dim = config.get(\"embedding_dim\", None)\n","        self.embedding_dim = embedding_dim if embedding_dim is not None else self.hidden_size\n","        self.embedding = nn.Embedding(config.get(\"n_classes\", 32), self.embedding_dim, padding_idx=0)\n","        gru_input_dim = self.embedding_dim\n","        self.rnn = nn.GRU(\n","            gru_input_dim,\n","            self.hidden_size,\n","            self.layers,\n","            dropout=self.dropout,\n","            bidirectional=self.bi,\n","            batch_first=True)# model 선언 \n","        self.gpu = config.get(\"gpu\", False) \n","\n","\n","\n","    def forward(self, inputs, hidden, input_lengths):\n","        \n","        # pack padded 를 통하여 input을 감싸기 \n","        inputs = self.embedding(inputs)\n","        \n","        x = pack_padded_sequence(inputs, input_lengths, batch_first=True) # ??? 무엇일까?\n","        output, state = self.rnn(x, hidden)\n","        output, _ = pad_packed_sequence(output, batch_first=True, padding_value=0.) # sequence 를 위의 그림과 같이 pack함 \n","        \n","        if self.bi: # bidirectional 의 경우 forward와 backward를 sum하여 사용한다. or concat \n","            output = output[:, :, :self.hidden_size] + output[:, :, self.hidden_size:]\n","            state = state[:1] +state[1:]\n","        return output, state\n","\n","    def init_hidden(self, batch_size):\n","        # hidden state가 없는 초기 상태일때 \n","        h0 = torch.zeros(2 if self.bi else 1, batch_size, self.hidden_size)\n","        if self.gpu:\n","            h0 = h0.cuda()\n","        return h0"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aZSYZqWVwM_z","colab_type":"text"},"source":["![대체 텍스트](https://pytorch.org/tutorials/_images/attention-decoder-network.png)"]},{"cell_type":"markdown","metadata":{"id":"l8yDzSXKHnyB","colab_type":"text"},"source":["### Decoder "]},{"cell_type":"code","metadata":{"id":"X83Lj_GBHojW","colab_type":"code","colab":{}},"source":["class Decoder(nn.Module):\n","    def __init__(self, config):\n","        super(Decoder, self).__init__()\n","        self.batch_size = config[\"batch_size\"]\n","        self.hidden_size = config[\"decoder_hidden\"]\n","        embedding_dim = config.get(\"embedding_dim\", None)\n","        self.embedding_dim = embedding_dim if embedding_dim is not None else self.hidden_size\n","        self.embedding = nn.Embedding(config.get(\"n_classes\", 32), self.embedding_dim, padding_idx=0)\n","        self.rnn = nn.GRU(\n","            input_size=self.embedding_dim+self.hidden_size if config['decoder'].lower() == 'bahdanau' else self.embedding_dim,\n","            hidden_size=self.hidden_size,\n","            num_layers=config.get(\"decoder_layers\", 1),\n","            dropout=config.get(\"decoder_dropout\", 0),\n","            bidirectional=False,\n","            batch_first=True)\n","        if config['decoder'] != \"RNN\":\n","            self.attention = Attention(\n","                self.batch_size,\n","                self.hidden_size,\n","                method=config.get(\"attention_score\", \"dot\"))\n","\n","        self.gpu = config.get(\"gpu\", False)\n","        self.decoder_output_fn = F.log_softmax if config.get('loss', 'NLL') == 'NLL' else None\n","\n","    def forward(self, **kwargs):\n","        \"\"\" Must be overrided \"\"\"\n","        raise NotImplementedError"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jcUzMxqV9Lr2","colab_type":"text"},"source":["![대체 텍스트](https://i.ibb.co/gvpn1RT/bmm.png)"]},{"cell_type":"markdown","metadata":{"id":"CBkw-82hE3MU","colab_type":"text"},"source":["![대체 텍스트](http://cnyah.com/2017/08/01/attention-variants/attention-mechanisms.png)"]},{"cell_type":"code","metadata":{"id":"IYbzCRP6HuLD","colab_type":"code","colab":{}},"source":["class BahdanauDecoder(Decoder):\n","    \"\"\"\n","        Corresponds to BahdanauAttnDecoderRNN in Pytorch tutorial\n","    \"\"\"\n","\n","    def __init__(self, config):\n","        super(BahdanauDecoder, self).__init__(config)\n","        self.output_size = config.get(\"n_classes\", 32)\n","        self.character_distribution = nn.Linear(self.hidden_size, self.output_size)\n","\n","    def forward(self, **kwargs):\n","        \"\"\"\n","        :param input: [B]\n","        :param prev_context: [B, H]\n","        :param prev_hidden: [B, H]\n","        :param encoder_outputs: [B, T, H]\n","        :return: output (B), context (B, H), prev_hidden (B, H), weights (B, T)\n","        \"\"\"\n"," \n","        input = kwargs[\"input\"] # decoder input \n","        prev_hidden = kwargs[\"prev_hidden\"] # decoder rnn 에서 들어갈 previous hidden state \n","        encoder_outputs = kwargs[\"encoder_outputs\"] # encoder RNN에서 Encoding이 끝난 (B,L,hidden_size)  \n","        seq_len = kwargs.get(\"seq_len\", None) # sequence length \n","\n","        # check inputs\n","        \n","       \n","\n","        # Attention weights\n","        weights = self.attention.forward(prev_hidden, encoder_outputs, seq_len)  # B x T => [B,1,T]\n","        context = weights.unsqueeze(1).bmm(encoder_outputs).squeeze(1)  #[B,1,T] [B,T,H]-> B,1,H -> [B x H]\n","\n","        # embed characters\n","        embedded = self.embedding(input).unsqueeze(0) #(B,EMBEDDING) -> (1,B,EMBEDDING)\n","        \n","        #attention 을 통해 얻어낸 context를 추가하여 모델에 input으로 제공\n","        rnn_input = torch.cat((embedded, context.unsqueeze(0)), 2) # (1,B,EMBEDDING) (1,B,H) -> (1,B,EMB+H)\n","        # (1,b,emb+h) => .transpose(1,0) -> (b,1,emb+h)\n","        outputs, hidden = self.rnn(rnn_input.transpose(1, 0), prev_hidden.unsqueeze(0)) # 1 x B x N, B x N\n","\n","        # (b,1,hidden_size) => (b,logits)\n","        output = self.character_distribution(outputs.squeeze(1)) # logit 값 각 chracter 별로\n","\n","        if self.decoder_output_fn:\n","            # NLL loss 인 경우 \n","            output = self.decoder_output_fn(output, -1)\n","\n","        if len(output.size()) == 3:\n","            output = output.squeeze(1)\n","\n","        return output, hidden.squeeze(0), weights\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eHhtZXhUEQ64","colab_type":"text"},"source":["![대체 텍스트](https://i.stack.imgur.com/tiQkz.png)"]},{"cell_type":"code","metadata":{"id":"kYfGqtu0eqZy","colab_type":"code","colab":{}},"source":["class Attention(nn.Module):\n","    \"\"\"\n","    Inputs:\n","        last_hidden: (batch_size, hidden_size)\n","        encoder_outputs: (batch_size, max_time, hidden_size)\n","    Returns:\n","        attention_weights: (batch_size, max_time)\n","    \"\"\"\n","    def __init__(self, batch_size, hidden_size, method=\"dot\"):\n","        super(Attention, self).__init__()\n","        self.method = method\n","        self.hidden_size = hidden_size\n","        if method == 'dot':\n","            pass\n","        elif method == 'general':\n","            # Wa (hidden,hidden)\n","            self.Wa = nn.Linear(hidden_size, hidden_size, bias=False)\n","        elif method == \"concat\":\n","            # Wa : (2*hidden,hidden)\n","            # Va : (hidden,1)\n","            self.Wa = nn.Linear(2*hidden_size, hidden_size, bias=False)\n","            self.va = nn.Parameter(torch.FloatTensor(hidden_size, 1))\n","        elif method == 'bahdanau':\n","            # Wa : (hidden_size,hidden_size) \n","            # Ua : (hidden_size,hidden_size)\n","            # Va : (hidden_size,1)\n","            self.Wa = nn.Linear(hidden_size, hidden_size, bias=False) # \n","            self.Ua = nn.Linear(hidden_size, hidden_size, bias=False)\n","            self.va = nn.Parameter(torch.FloatTensor(hidden_size, 1))\n","        else:\n","            raise NotImplementedError\n","\n","        \n","    def forward(self, last_hidden, encoder_outputs, seq_len=None):\n","        \"\"\"\n","        Inputs :\n","          last_hidden : (B,T,hidden_size)\n","          encoder_outputs : \n","          seq_len:  \n","        Returns:\n","          attention matrix : \n","        \"\"\"\n","        batch_size, seq_lens, _ = encoder_outputs.size()\n","        # attention energies 를 구하기 \n","        attention_energies = self._score(last_hidden, encoder_outputs, self.method)\n","        \n","        if seq_len is not None:\n","            attention_energies = mask_3d(attention_energies, seq_len, -float('inf'))\n","\n","        return F.softmax(attention_energies, -1)\n","\n","    def _score(self, last_hidden, encoder_outputs, method):\n","        \"\"\"\n","        Computes an attention score\n","        :param last_hidden: (batch_size, hidden_dim)\n","        :param encoder_outputs: (batch_size, max_time, hidden_dim)\n","        :param method: str (`dot`, `general`, `concat`)\n","        :return:\n","        \"\"\"\n","\n","        # assert last_hidden.size() == torch.Size([batch_size, self.hidden_size]), last_hidden.size()\n","        \n","        if method == 'dot':\n","            last_hidden = last_hidden.unsqueeze(-1) # (batch_size, hidden_dim,1)\n","            \n","            # attention : (batch_size,max_time, hidden_dim) , (batch_size,hidden_dim,1) - > (batch_size,max_time ,1)\n","            # batchsize,max_timstep\n","            return encoder_output.bmm(last_hidden).squeeze(-1)  \n","\n","        elif method == 'general':\n","            # dot 이랑 비슷 다만 last hidden을 한번 projection\n","            x = self.Wa(last_hidden) # (batch_size, hidden_dim) ->  (batch_size, hidden_dim)\n","            x = x.unsqueeze(-1) # (batch_size, hidden_dim) ->  (batch_size, hidden_dim,1)\n","            # encoded 된 hidden states 와 dot proudct를 수행하기 \n","            # attention: (batch_size,max_time, hidden_dim) , (batch_size,hidden_dim,1) - > (batch_size,max_time ,1)\n","            return encoder_output.bmm(x).squeeze(-1)\n","\n","        elif method == \"concat\":\n","            # (b,max_time,hidden_state) (b,hidden_state) #  -> unsqueeze -> (b,1,hidde_Size) -> b,max_time,hidden_state\n","            x = last_hidden.unsqueeze(1).expand_as(encoder_outputs) # (batch_size, hidden_dim) ->  (batch_size,1, hidden_dim)\n","            # concat 후 -> linear 거치기 -> 후 tanh\n","            x = F.tanh(self.Wa(torch.cat((x, encoder_outputs), -1))) # (batch_size, max_timestep, hidden_dim) ->  (batch_size,  max_timestep, hidden_dim*2)\n","            # (batch_size, max_timestep, hidden_dim*2) ->  (batch_size,  max_timestep,1) => squeeze(-1)=> (b,max)\n","            return x.matmul(self.va).squeeze(-1)\n","\n","        elif method == \"bahdanau\":\n","            # mlp 기반의 attention model\n","            a\n","            #x = last_hidden.unsqueeze(1) # (batch_size, hidden_dim) ->  (batch_size,1, hidden_dim)\n","            # 각각을 projection 후 더하기 -> tanh \n","            #out = F.???(self.??(x) ?? self.???(encoder_outputs)) # \n","            #return ??.matmul(self.va).squeeze(-1)# (batch_size,max_timestep,hidden_dim) ->  (batch_size, max_timestep)\n","\n","        else:\n","            raise NotImplementedError"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JqeFcsDJ7Dz2","colab_type":"text"},"source":["### Seq2Seq Model "]},{"cell_type":"markdown","metadata":{"id":"jS2cmY548vtB","colab_type":"text"},"source":["![대체 텍스트](https://i.ibb.co/CK5wTz5/crossentropy.png)"]},{"cell_type":"markdown","metadata":{"id":"z_rIN2DY80pD","colab_type":"text"},"source":["![대체 텍스트](https://i.ibb.co/ssXZ28q/crossentropy-2.png)"]},{"cell_type":"code","metadata":{"id":"V-axgHRo7C5V","colab_type":"code","colab":{}},"source":["class Seq2Seq(nn.Module):\n","    \"\"\"\n","        Sequence to sequence module\n","    \"\"\"\n","\n","    def __init__(self, config):\n","        super(Seq2Seq, self).__init__()\n","        self.SOS = config.get(\"start_index\", 1) # Start index를 가져옵니다. \n","        self.vocab_size = config.get(\"n_classes\", 32) # embedding 에 필요한 vocabulary size \n","        self.batch_size = config.get(\"batch_size\", 1) # batch_size 정보를 가져옵니다.\n","        self.gpu = config.get(\"gpu\", False) # cuda 로 돌아가는지 아닌지에 대한 정보 \n","\n","        # Encoder 선언\n","        \n","        self.encoder = EncoderRNN(config)\n","\n","        # Decoder 선언 \n","        \n","        self.decoder = BahdanauDecoder(config)\n","        \n","        # loss fucntion \n","        # ignore_index =0 왜???\n","        self.loss_fn = torch.nn.CrossEntropyLoss(ignore_index=0)\n","        \n","        \n","\n","    def encode(self, x, x_len):\n","        # encoder를 통해 주어진 source 정보를 Encodeing 하는 용도 \n","        \n","        batch_size = x.size()[0]\n","        # 초기 inital hidden state 만들기\n","        init_state = self.encoder.init_hidden(batch_size)\n","        # encoder Forward 수행 \n","        encoder_outputs, encoder_state = self.encoder.forward(x, init_state, x_len)\n","        \n","        \n","       \n","        return encoder_outputs, encoder_state.squeeze(0)\n","\n","    def decode(self, encoder_outputs, encoder_hidden, targets, targets_lengths, input_lengths):\n","        \"\"\"\n","        Args:\n","            encoder_outputs: (B, T, H)\n","            encoder_hidden: (B, H)\n","            targets: (B, L)\n","            targets_lengths: (B)\n","            input_lengths: (B)\n","        Vars:\n","            decoder_input: (B)\n","            decoder_context: (B, H)\n","            hidden_state: (B, H)\n","            attention_weights: (B, T)\n","        Outputs:\n","            alignments: (L, T, B)\n","            logits: (B*L, V)\n","            labels: (B*L)\n","        \"\"\"\n","\n","        batch_size = encoder_outputs.size()[0]\n","        max_length = targets.size()[1]\n","        # decoder의 처음 y0 는 무엇이 되어야 할까? *주의해야할 포인트 \n","        if batch_size ==1:\n","          decoder_input = torch.LongTensor([self.SOS] * batch_size)\n","        else:\n","          decoder_input = torch.LongTensor([self.SOS] * batch_size).squeeze(-1)\n","        decoder_context = encoder_outputs.transpose(1, 0)[-1] #(Batch,1)\n","        decoder_hidden = encoder_hidden\n","        \n","        #alignments :  attention align을 저장하기 위한 용도  \n","        alignments = torch.zeros(max_length, encoder_outputs.size(1), batch_size) # attention align을 저장하기 위한 용도 \n","        logits = torch.zeros(max_length, batch_size, self.decoder.output_size) # logits 값을 저장하기 위한 용도의 tensor \n","\n","        if self.gpu:\n","            decoder_input = decoder_input.cuda()\n","            decoder_context = decoder_context.cuda()\n","            logits = logits.cuda()\n","        inference = []\n","        for t in range(max_length):\n","\n","            # The decoder accepts, at each time step t :\n","            # - an input, [B]\n","            # - a context, [B, H]\n","            # - an hidden state, [B, H]\n","            # - encoder outputs, [B, T, H]\n","            \n","            # The decoder outputs, at each time step t :\n","            # - an output, [B]\n","            # - a context, [B, H]\n","            # - an hidden state, [B, H]\n","            # - weights, [B, T]\n","\n","            outputs, decoder_hidden, attention_weights = self.decoder.forward(\n","                    input=decoder_input.long(),\n","                    prev_hidden=decoder_hidden,\n","                    encoder_outputs=encoder_outputs,\n","                    seq_len=input_lengths)\n","            \n","            alignments[t] = attention_weights.transpose(1, 0)\n","            \n","            \n","            logits[t] = outputs\n","\n","            \n","\n","            if  self.training:\n","                decoder_input = targets[:, t]\n","            else:\n","                topv, topi = outputs.data.topk(1) # 가장 높은 예측만 사용.\n","                decoder_input = topi.squeeze(-1).detach()\n","                inference.append(decoder_input.cpu())\n","\n","        \n","        labels = targets.contiguous().view(-1) \n","\n","        \n","        mask_value = 0\n","        #what is this mask_3d? # (warning check)\n","        logits = mask_3d(logits.transpose(1, 0), targets_lengths, mask_value)\n","        logits = logits.contiguous().view(-1, self.vocab_size) # loss를 구하기 위해 쫙 펴주기 \n","\n","        return logits, labels.long(), alignments,inference\n","\n","    \n","    def step(self, batch):\n","        x, y, x_len, y_len = batch\n","        if self.gpu:\n","            x = x.cuda()\n","            y = y.cuda()\n","            x_len = x_len.cuda()\n","            y_len = y_len.cuda()\n","\n","        encoder_out, encoder_state = self.encode(x, x_len) # encoder \n","        logits, labels, alignments,inference = self.decode(encoder_out, encoder_state, y, y_len, x_len) # decoder 를 통해 alignment와 logit 값 얻기 \n","        return logits, labels, alignments,inference\n","\n","    def loss(self, batch):\n","        logits, labels, alignments,inference = self.step(batch)\n","        loss = self.loss_fn(logits, labels) # loss 구하기 우리는 cross entropy 사용 \n","        return loss, logits, labels, alignments,inference"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xbOfjOmU6262","colab_type":"text"},"source":["### Train the model"]},{"cell_type":"markdown","metadata":{"id":"weB4hLTwfV3S","colab_type":"text"},"source":["![대체 텍스트](https://pytorch.org/tutorials/_images/seq2seq.png)"]},{"cell_type":"code","metadata":{"id":"NKYVKohKBmpS","colab_type":"code","colab":{}},"source":["def train(model, optimizer, train_loader, epoch,n_epochs):\n","    \n","\n","    losses = []\n","    cers = []\n","\n","    \n","    model.train() # train mode \n","    count = 0\n","    for batch in train_loader:\n","        loss, _, _, _,_ = model.loss(batch)\n","        losses.append(loss.item())\n","        # Reset gradients\n","        optimizer.zero_grad()\n","        # Compute gradients\n","        loss.backward()\n","        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2)\n","        optimizer.step()\n","  \n","    print ('\\n [{}/{}] avg_loss= {:05.3f}'.format(epoch,n_epochs,np.mean(losses)))\n","    \n","    return model, optimizer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"I0q5dl-qGtXB","colab_type":"code","colab":{}},"source":["def evaluate(model, eval_loader):\n","\n","    losses = []\n","    accs = []\n","    edits = []\n","    \n","    model.eval() # why?? \n","\n","    with torch.no_grad():\n","        for batch in eval_loader:\n","            #t.set_description(\" Evaluating... (train={})\".format(model.training))\n","            loss, logits, labels, alignments,_ = model.loss(batch)\n","            preds = logits.detach().cpu().numpy()\n","            \n","            acc = 100 *np.sum(np.argmax(preds, -1) == labels.detach().cpu().numpy()) / len(preds)\n","            edit = editdistance.eval(np.argmax(preds, -1), labels.detach().cpu().numpy()) / len(preds)\n","            \n","            losses.append(loss.item())\n","            \n","            accs.append(acc)\n","            edits.append(edit)\n","        \n","        \n","\n","   \n","    print(\"  End of evaluation : loss {:05.3f} , acc {:03.1f} , edits {:03.3f}\".format(np.mean(losses), np.mean(accs), np.mean(edits)))\n","    \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GWWvvbf68__6","colab_type":"text"},"source":["## 학습을 진행해보도록 하겠습니다"]},{"cell_type":"code","metadata":{"id":"N6zoFKQj8_MU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"f064092f-f2e8-4bd0-8405-c7002b975ce0","executionInfo":{"status":"ok","timestamp":1564648202293,"user_tz":-540,"elapsed":6420,"user":{"displayName":"Cheonbok Park","photoUrl":"https://lh3.googleusercontent.com/-IBOYs9WOjak/AAAAAAAAAAI/AAAAAAAABAk/gDfiXuRJUhA/s64/photo.jpg","userId":"16050539117346266781"}}},"source":["USE_CUDA = torch.cuda.is_available()\n","batch_size = 32\n","epochs = 6\n","dataset = ToyDataset(5, 15)\n","eval_dataset = ToyDataset(5, 15, type='eval')"],"execution_count":57,"outputs":[{"output_type":"stream","text":["{'a': 3, 'b': 4, 'c': 5, 'd': 6, 'e': 7}\n","{'a': 3, 'b': 4, 'c': 5, 'd': 6, 'e': 7}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PKechkHv9EXQ","colab_type":"code","colab":{}},"source":["train_loader = data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate, drop_last=True)\n","eval_loader = data.DataLoader(eval_dataset, batch_size=batch_size, shuffle=False, collate_fn=pad_collate,drop_last=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JapBBgq6KNp2","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"xh2HQkg59sND","colab_type":"code","colab":{}},"source":["config = {\n","  \"decoder\": \"Bahdanau\",\n","  \"encoder\": \"RNN\",\n","  \"n_channels\": 5,\n","  \"encoder_hidden\": 64,\n","  \"encoder_layers\": 1,\n","  \"encoder_dropout\": 0.2,\n","  \"bidirectional_encoder\": False,\n","  \"decoder_hidden\": 64,\n","  \"decoder_layers\": 1,\n","  \"decoder_dropout\": 0.2,\n","  \"n_classes\":dataset.VOCAB_SIZE+3 ,\n","  \"batch_size\": 32,\n","  \"embedding_dim\": 64,\n","  \"attention_score\": \"concat\",\n","  \"learning_rate\": 0.001,\n","  \"gpu\": True,\n","  \"loss\": \"cross_entropy\"\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N_h_LOmU-AEz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"outputId":"0ce04a0b-fdfa-4505-b464-4093b303c09c","executionInfo":{"status":"ok","timestamp":1564648202295,"user_tz":-540,"elapsed":4876,"user":{"displayName":"Cheonbok Park","photoUrl":"https://lh3.googleusercontent.com/-IBOYs9WOjak/AAAAAAAAAAI/AAAAAAAABAk/gDfiXuRJUhA/s64/photo.jpg","userId":"16050539117346266781"}}},"source":["model = Seq2Seq(config)\n","model = model.cuda()"],"execution_count":60,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"fxsUu4elTL29","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"_jXTGn9V-CuD","colab_type":"code","colab":{}},"source":["optimizer = torch.optim.Adam(model.parameters(), lr=config.get(\"learning_rate\", .001))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ktMBioF-EAN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":377},"outputId":"46a459fe-d53d-4a7a-d752-f275f2372812","executionInfo":{"status":"ok","timestamp":1564648345674,"user_tz":-540,"elapsed":146385,"user":{"displayName":"Cheonbok Park","photoUrl":"https://lh3.googleusercontent.com/-IBOYs9WOjak/AAAAAAAAAAI/AAAAAAAABAk/gDfiXuRJUhA/s64/photo.jpg","userId":"16050539117346266781"}}},"source":["for epoch in range(epochs):\n","  model,optimizer  = train(model,optimizer, train_loader,epoch,epochs)\n","  evaluate(model,eval_loader)\n"," "],"execution_count":62,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["\n"," [0/6] avg_loss= 1.366\n","  End of evaluation : loss 1.301 , acc 55.7 , edits 0.352\n","\n"," [1/6] avg_loss= 0.962\n","  End of evaluation : loss 1.170 , acc 63.2 , edits 0.294\n","\n"," [2/6] avg_loss= 0.734\n","  End of evaluation : loss 1.082 , acc 68.5 , edits 0.252\n","\n"," [3/6] avg_loss= 0.609\n","  End of evaluation : loss 1.027 , acc 71.6 , edits 0.230\n","\n"," [4/6] avg_loss= 0.518\n","  End of evaluation : loss 0.967 , acc 74.8 , edits 0.201\n","\n"," [5/6] avg_loss= 0.467\n","  End of evaluation : loss 0.918 , acc 75.3 , edits 0.192\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5RNRT6acajT5","colab_type":"text"},"source":["### 시각화"]},{"cell_type":"code","metadata":{"id":"cQ77ZlPKal2j","colab_type":"code","colab":{}},"source":["import seaborn\n","\n","def draw(data, x, y):\n","    seaborn.heatmap(data, \n","                    xticklabels=x, square=True, yticklabels=y, vmin=0.0, vmax=1.0, \n","                    cbar=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h3fVzxjganHg","colab_type":"code","colab":{}},"source":["def visualize_plot(model,custom_input= 'cgdafa'):\n","    c_xs = np.array([dataset.char2int.get(x) for x in custom_input]+[2])\n","    c_xs = torch.from_numpy(c_xs).unsqueeze(0).long()\n","\n","    c_xl = torch.tensor(c_xs[0].size()[-1]).unsqueeze(0)\n","\n","    c_ys = np.array([dataset.char2int.get(x) for x in custom_input[::-1]] + [2]) # Return the random string and its reverse + EOS \n","    c_ys = torch.from_numpy(c_ys).unsqueeze(0).long()\n","\n","    c_yl = torch.tensor(c_ys[0].size()[-1]).unsqueeze(0)\n","    c_data = (c_xs,c_ys,c_xl,c_yl)\n","    loss, logits, labels, alignments,predict=model.loss(c_data)\n","    heat_map_value = alignments.detach().cpu().numpy()[:, :, 0]\n","    preds = logits.detach().cpu().numpy()\n","    preds = np.argmax(preds, -1)\n","    source_tokens = [ dataset.int2char[item-3] for item in c_xs[0] if item!=0 if item !=2 ] +['</s>']\n","    target_tokens = [ dataset.int2char[item-3] if item !=2 else '</s>' for item in preds.tolist() if item!=0 ]\n","    draw(heat_map_value,source_tokens,target_tokens)\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4IkbDmwaY4BG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":323},"outputId":"71acfd53-1e4a-458b-e891-03deeee64e07","executionInfo":{"status":"ok","timestamp":1564648345677,"user_tz":-540,"elapsed":143251,"user":{"displayName":"Cheonbok Park","photoUrl":"https://lh3.googleusercontent.com/-IBOYs9WOjak/AAAAAAAAAAI/AAAAAAAABAk/gDfiXuRJUhA/s64/photo.jpg","userId":"16050539117346266781"}}},"source":["visualize_plot(model,'cbada')"],"execution_count":65,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAPwAAAD8CAYAAABTq8lnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"ZZdAsEpjcC0V","colab_type":"text"},"source":["### Dot Mode "]},{"cell_type":"code","metadata":{"id":"T8X3wWrrewkY","colab_type":"code","colab":{}},"source":["config = {\n","  \"decoder\": \"Bahdanau\",\n","  \"encoder\": \"RNN\",\n","  \"n_channels\": 4,\n","  \"encoder_hidden\": 64,\n","  \"encoder_layers\": 1,\n","  \"encoder_dropout\": 0.2,\n","  \"bidirectional_encoder\": False,\n","  \"decoder_hidden\": 64,\n","  \"decoder_layers\": 1,\n","  \"decoder_dropout\": 0.2,\n","  \"n_classes\":dataset.VOCAB_SIZE+3 ,\n","  \"batch_size\": 32,\n","  \"embedding_dim\": 64,\n","  \"attention_score\": \"concat\",\n","  \"learning_rate\": 0.001,\n","  \"gpu\": True,\n","  \"loss\": \"cross_entropy\"\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8xHTy6m9k7XU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"outputId":"ccf4f389-851e-4726-d105-77f28fe8ef4c","executionInfo":{"status":"ok","timestamp":1564647917639,"user_tz":-540,"elapsed":550,"user":{"displayName":"Cheonbok Park","photoUrl":"https://lh3.googleusercontent.com/-IBOYs9WOjak/AAAAAAAAAAI/AAAAAAAABAk/gDfiXuRJUhA/s64/photo.jpg","userId":"16050539117346266781"}}},"source":["model = Seq2Seq(config)\n","model = model.cuda()"],"execution_count":48,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"LuAGn6wOcMAU","colab_type":"code","colab":{}},"source":["optimizer = torch.optim.Adam(model.parameters(), lr=config.get(\"learning_rate\", .001))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6NNv1KexcOQh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":377},"outputId":"b56f1e50-9d50-4de3-ca27-1e3a235edc46","executionInfo":{"status":"ok","timestamp":1564648060598,"user_tz":-540,"elapsed":141594,"user":{"displayName":"Cheonbok Park","photoUrl":"https://lh3.googleusercontent.com/-IBOYs9WOjak/AAAAAAAAAAI/AAAAAAAABAk/gDfiXuRJUhA/s64/photo.jpg","userId":"16050539117346266781"}}},"source":["for epoch in range(epochs):\n","  model,optimizer  = train(model,optimizer, train_loader,epoch,epochs)\n","  evaluate(model,eval_loader)\n"," "],"execution_count":50,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["\n"," [0/6] avg_loss= 1.394\n","  End of evaluation : loss 1.324 , acc 57.4 , edits 0.340\n","\n"," [1/6] avg_loss= 0.986\n","  End of evaluation : loss 1.202 , acc 63.5 , edits 0.295\n","\n"," [2/6] avg_loss= 0.759\n","  End of evaluation : loss 1.101 , acc 68.6 , edits 0.248\n","\n"," [3/6] avg_loss= 0.611\n","  End of evaluation : loss 1.070 , acc 71.2 , edits 0.230\n","\n"," [4/6] avg_loss= 0.573\n","  End of evaluation : loss 1.083 , acc 71.8 , edits 0.227\n","\n"," [5/6] avg_loss= 0.555\n","  End of evaluation : loss 1.075 , acc 72.5 , edits 0.222\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"48ACZ__zcgz3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":323},"outputId":"6b73e699-664a-48e0-d25b-2047c3760cb1","executionInfo":{"status":"ok","timestamp":1564648060599,"user_tz":-540,"elapsed":141151,"user":{"displayName":"Cheonbok Park","photoUrl":"https://lh3.googleusercontent.com/-IBOYs9WOjak/AAAAAAAAAAI/AAAAAAAABAk/gDfiXuRJUhA/s64/photo.jpg","userId":"16050539117346266781"}}},"source":["visualize_plot(model,'cbada')"],"execution_count":51,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAPwAAAD8CAYAAABTq8lnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"jZio7ZpScXWf","colab_type":"text"},"source":["### Concat Mode "]},{"cell_type":"code","metadata":{"id":"rlEMn4CDcVdv","colab_type":"code","colab":{}},"source":["config = {\n","  \"decoder\": \"Bahdanau\",\n","  \"encoder\": \"RNN\",\n","  \"n_channels\": 4,\n","  \"encoder_hidden\": 64,\n","  \"encoder_layers\": 1,\n","  \"encoder_dropout\": 0.2,\n","  \"bidirectional_encoder\": False,\n","  \"decoder_hidden\": 64,\n","  \"decoder_layers\": 1,\n","  \"decoder_dropout\": 0.2,\n","  \"n_classes\":dataset.VOCAB_SIZE+3 ,\n","  \"batch_size\": 32,\n","  \"embedding_dim\": 64,\n","  \"attention_score\": \"concat\",\n","  \"learning_rate\": 0.001,\n","  \"gpu\": True,\n","  \"loss\": \"cross_entropy\"\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KvMOke61cbvf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"outputId":"04b5e685-7e63-447b-bfae-67bf4ab99de5","executionInfo":{"status":"ok","timestamp":1564648060600,"user_tz":-540,"elapsed":138988,"user":{"displayName":"Cheonbok Park","photoUrl":"https://lh3.googleusercontent.com/-IBOYs9WOjak/AAAAAAAAAAI/AAAAAAAABAk/gDfiXuRJUhA/s64/photo.jpg","userId":"16050539117346266781"}}},"source":["model = Seq2Seq(config)\n","model = model.cuda()"],"execution_count":53,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"xCRpTxKVcc__","colab_type":"code","colab":{}},"source":["optimizer = torch.optim.Adam(model.parameters(), lr=config.get(\"learning_rate\", .001))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mjbdwe3jcfFD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":377},"outputId":"f93a00be-8940-40dd-e794-79b0275ef69f","executionInfo":{"status":"ok","timestamp":1564648202027,"user_tz":-540,"elapsed":279166,"user":{"displayName":"Cheonbok Park","photoUrl":"https://lh3.googleusercontent.com/-IBOYs9WOjak/AAAAAAAAAAI/AAAAAAAABAk/gDfiXuRJUhA/s64/photo.jpg","userId":"16050539117346266781"}}},"source":["for epoch in range(epochs):\n","  model,optimizer  = train(model,optimizer, train_loader,epoch,epochs)\n","  evaluate(model,eval_loader)\n"," "],"execution_count":55,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["\n"," [0/6] avg_loss= 1.381\n","  End of evaluation : loss 1.294 , acc 57.8 , edits 0.334\n","\n"," [1/6] avg_loss= 0.925\n","  End of evaluation : loss 1.120 , acc 66.6 , edits 0.262\n","\n"," [2/6] avg_loss= 0.697\n","  End of evaluation : loss 1.045 , acc 70.6 , edits 0.235\n","\n"," [3/6] avg_loss= 0.583\n","  End of evaluation : loss 0.972 , acc 74.3 , edits 0.207\n","\n"," [4/6] avg_loss= 0.499\n","  End of evaluation : loss 0.918 , acc 76.0 , edits 0.191\n","\n"," [5/6] avg_loss= 0.462\n","  End of evaluation : loss 0.931 , acc 76.8 , edits 0.188\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uhnkBB82chid","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":323},"outputId":"9d0cf28b-f930-445a-be86-5cac183bbb01","executionInfo":{"status":"ok","timestamp":1564648202028,"user_tz":-540,"elapsed":278537,"user":{"displayName":"Cheonbok Park","photoUrl":"https://lh3.googleusercontent.com/-IBOYs9WOjak/AAAAAAAAAAI/AAAAAAAABAk/gDfiXuRJUhA/s64/photo.jpg","userId":"16050539117346266781"}}},"source":["visualize_plot(model,'cbada')"],"execution_count":56,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAPwAAAD8CAYAAABTq8lnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHPzVdvIAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"uMzGeSiUdZRo","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}